{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ua-datalab/NLP-Speech/blob/main/Text_pre_processing_for_NLP/Text_pre_processing_for_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center> Text Pre Processing for NLP\n",
        "\n",
        "\n",
        "<img src=\"https://images.datacamp.com/image/upload/v1669223212/Text_Mining_6eeff5cb7c.png\" width=\"800\"/>\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "Y1Qe19Kv212z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Housekeeping\n",
        "1. Check that the recording is on\n",
        "2. Check audio and screenshare\n",
        "3. Share link to notebook in chat\n",
        "4. Check for light mode and readable font size"
      ],
      "metadata": {
        "id": "h00WlRlEOwuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Text Pre-processing?\n",
        "\n",
        "Text preprocessing involves **cleaning and preparing raw text data** so that it can be passed on to downstream NLP tasks, such as text analysis or model training. When conducted effectively, text preprocessing can significantly impact the performance and accuracy of NLP models.\n",
        "\n",
        "Today, we will look at some the essential steps involved in text preprocessing for NLP tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "_ntT6SjC3BNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Text Preprocessing is Important?\n",
        "\n",
        "Raw text data is often:\n",
        "- **Noisy**: The contents could include inconsistencies such as typos, slang, abbreviations\n",
        "- **Unstructured**: Itvexists in the form of characters, that are not machine-readable.\n",
        "\n",
        "Preprocessing helps in:\n",
        "\n",
        "- **Improving Data Quality:**\n",
        "By removing noise and irrelevant components, the processed data is made clean and consistent.\n",
        "\n",
        "- **Enhancing Model Performance:**\n",
        "When a dataset is well-preprocessed, it improves feature extraction. Which, in turn improves the performance of NLP models.\n",
        "\n",
        "- **Reducing Complexity:**\n",
        "By narrowing the dataset to relevant text elements, we can reduce the required computational overhead, and make the training of models more efficient.\n"
      ],
      "metadata": {
        "id": "-zazPxFMPLEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Today's workshop will focus on 4 use-cases for text-preprocessing, and we will work with:\n",
        "\n",
        "- **Plain Text**\n",
        "- **Web page**\n",
        "- **Pdf files**\n",
        "\n"
      ],
      "metadata": {
        "id": "GRiSXV8C0RTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plain Text: Simple Text Preprocessing Techniques:"
      ],
      "metadata": {
        "id": "IaWNRIKmvfUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Text Processing\n",
        "\n",
        "In this example, we will work on the following tasks:\n",
        "- Converting text to lowercase\n",
        "- Removing punctuation and special characters (reducing special characters)\n",
        "- Removing numbers\n",
        "\n",
        "These tasks can be carried out with basic python functions and regular expressions. More details for ReGex for NLP is available in our previous [workshop notebook](https://github.com/ua-datalab/NLP-Speech/blob/main/Introduction_to_Regular_Expressions/Introduction_to_Regular_Expressions.ipynb)"
      ],
      "metadata": {
        "id": "HiPyuBBtviHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"I can't wait for the new season of my favorite show!\",\n",
        "    \"The COVID-19 pandemic has affected millions of people worldwide.\",\n",
        "    \"U.S. stocks fell on Friday after news of rising inflation.\",\n",
        "    \"Python is a great programming language ^-^ !!! ??\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "kV3gKyaQv7Th"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    return text\n",
        "\n",
        "cleaned_corpus = [clean_text(doc) for doc in corpus]\n",
        "\n",
        "print(*(item for item in cleaned_corpus), sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "_9Smf5uGvHnc",
        "outputId": "ad55145b-70e2-4757-dafd-3966bc6739e2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i cant wait for the new season of my favorite show\n",
            "the covid pandemic has affected millions of people worldwide\n",
            "us stocks fell on friday after news of rising inflation\n",
            "python is a great programming language   \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'expandtabs'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-95d5b91b0a76>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextwrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/textwrap.py\u001b[0m in \u001b[0;36mfill\u001b[0;34m(text, width, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[1;32m    398\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshorten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/textwrap.py\u001b[0m in \u001b[0;36mfill\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mwrapped\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \"\"\"\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/textwrap.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mconverted\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix_sentence_endings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fix_sentence_endings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/textwrap.py\u001b[0m in \u001b[0;36m_split_chunks\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_split_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_munge_whitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/textwrap.py\u001b[0m in \u001b[0;36m_munge_whitespace\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \"\"\"\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_tabs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpandtabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_whitespace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0municode_whitespace_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'expandtabs'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the NLP tasks, we will use the NLTK python library. These tasks can also be conducted using the SpaCy package.\n",
        "\n",
        "[See our previous workshop on NLP with SpaCy for more details](https://github.com/ua-datalab/NLP-Speech/blob/main/Natural_Language_Processing_Text_Mining_and_Sentiment_Analysis/Natural_Language_Processing_Text_Mining_and_Sentiment_Analysis.ipynb)."
      ],
      "metadata": {
        "id": "8ztjCs9qpVni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "1tQk_iUPsCI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus = [word_tokenize(doc) for doc in cleaned_corpus]\n",
        "print(len(tokenized_corpus))\n",
        "print(*(item for item in tokenized_corpus), sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcgN4cvCwkIO",
        "outputId": "f0e51554-d84a-4ec3-e711-4fcfc57f916f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "['i', 'cant', 'wait', 'for', 'the', 'new', 'season', 'of', 'my', 'favorite', 'show']\n",
            "['the', 'covid', 'pandemic', 'has', 'affected', 'millions', 'of', 'people', 'worldwide']\n",
            "['us', 'stocks', 'fell', 'on', 'friday', 'after', 'news', 'of', 'rising', 'inflation']\n",
            "['python', 'is', 'a', 'great', 'programming', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Words Removal\n",
        "\n",
        "Once we have limited our dataset to words, depending on the task, we might need only meaning-carrying words in our corpus. For tasks such as topic modeling, the objective involves looking for nouns and verbs, and excluding adverbs, articles and other grammatical elements of the text.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LT6sgW1QwMaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-Snrhpjs35a",
        "outputId": "ee16917f-2bc9-4150-fff5-d00d1b4d65b8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#create an element containing all the English stopwords:\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# iterate over each sentence, to save content words:\n",
        "filtered_corpus = [[word for word in doc if word not in stop_words] for doc in tokenized_corpus]\n",
        "print(*(item for item in filtered_corpus), sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyeByH0gwWba",
        "outputId": "048cd9ba-22a9-46fc-896c-c3a62de4ddea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cant', 'wait', 'new', 'season', 'favorite', 'show']\n",
            "['covid', 'pandemic', 'affected', 'millions', 'people', 'worldwide']\n",
            "['us', 'stocks', 'fell', 'friday', 'news', 'rising', 'inflation']\n",
            "['python', 'great', 'programming', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Lemmatization and Stemming\n",
        "\n",
        "While processing our documents, we may not want the model to be confused by words that have the same meaning but with different inflections (plural markers, tense markers, etc.). So we will remove the inflections and replace each token with its lemma. This uses a corpus called the WordNet, and its [NLTK library](https://www.nltk.org/howto/stem.html).\n",
        "\n",
        "Learn more about WordNet here: https://wordnet.princeton.edu/\n"
      ],
      "metadata": {
        "id": "4zT2z2mNwWym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qc8nHvItWrz",
        "outputId": "c58bd7a7-651f-4c3c-df96-cbbdc86e7a87"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed_corpus = [[stemmer.stem(word) for word in doc] for doc in filtered_corpus]\n",
        "lemmatized_corpus = [[lemmatizer.lemmatize(word) for word in doc] for doc in filtered_corpus]\n",
        "print(\"Stems:\")\n",
        "print(*(item for item in stemmed_corpus), sep='\\n')\n",
        "print(\"Lemmas:\")\n",
        "print(*(item for item in lemmatized_corpus), sep='\\n')"
      ],
      "metadata": {
        "id": "HWZ2MvzKwsa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: In this example, after stemming, some of the words may look strange or misspelt. Note that these are just the way they are stored by NLTK. This is done so NLTK can call those entries correctly."
      ],
      "metadata": {
        "id": "Tdur60SiukLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Handling Contractions\n",
        "Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe.\n",
        "\n",
        "Expanding contractions in the text.\n",
        "\n",
        "For example: ` I’ll be there within 5 minutes, won't you?`\n",
        "\n",
        "To process contractions, we will use the `contractions` library:"
      ],
      "metadata": {
        "id": "AgaVHlbTvGsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "aQnLoKNqxN4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import textwrap\n",
        "import contractions\n",
        "# contracted text\n",
        "text = '''I'll be there within 5 min. Shouldn't you be there too?\n",
        "\t\tI'd love to see u there my dear. It's awesome to meet new friends.\n",
        "\t\tWe've been waiting for this day for so long, lol.\n",
        "    Aren't you leaving today?'''\n",
        "\n",
        "# creating an empty list\n",
        "expanded_words = \"\"\n",
        "\n",
        "# using contractions.fix() to expand the shortened words\n",
        "for word in text.split():\n",
        "  expanded_words += contractions.fix(word) + \" \"\n",
        "\n",
        "print(\"Original text:\\n\" + textwrap.fill(text, width=70)+\"\\n\")\n",
        "print(\"Text without contractions:\\n\" +textwrap.fill(expanded_text, width=70))"
      ],
      "metadata": {
        "id": "mrmycsYIw45I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Parts of Speech (POS)\n",
        "\n",
        "In this task, we label each word in a sentence with its corresponding part of speech, such as noun, verb, adjective, etc.\n",
        "\n",
        "This information is crucial for many NLP applications, including parsing, information retrieval, and text analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "uDTLDlm9a2L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "ihKnYGoXyLqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the NLTK library\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Sample text\n",
        "text = \"NLTK is a powerful library for natural language processing.\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "\n",
        "# Performing PoS tagging\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "# Displaying the PoS tagged result in separate lines\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "\n",
        "print(\"\\nPoS Tagging Result:\")\n",
        "for word, pos_tag in pos_tags:\n",
        "\tprint(f\"{word}: {pos_tag}\")\n"
      ],
      "metadata": {
        "id": "4uR3BgIHyMyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing web pages\n",
        "\n",
        "If you get a web data dump, a considerable portion of it will be CSS adn HTML elements used for creating the front end. We need to extract the actual contents of the page. For this task, we can use the Beautiful Soup library, that can identify and process HTML tags.\n",
        "\n",
        "Often, the text in a page is stored under specific tags (such as `<body>` or `<article>`). We may want to preserve our text along with the tag it is saved under."
      ],
      "metadata": {
        "id": "GoEAeaJj632S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Extracting text from html pages using beautiful soup\n",
        "\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "8BWJRlVqyI3_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_html(html_content:str):\n",
        "  '''\n",
        "  searches each html tag, and returns the contents and a dictionary containing the tag name and\n",
        "  the text housed in each html tag\n",
        "  '''\n",
        "  # Initialize an empty dictionary\n",
        "  text_dict = {}\n",
        "\n",
        "  # Create a BeautifulSoup object\n",
        "  soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "  # Extract text, split by new line and full stops.\n",
        "  # This helps improve readability of real, complex web pages\n",
        "  text = soup.get_text(separator='\\n').strip()\n",
        "\n",
        "  # Loop through all tags and extract text\n",
        "  for tag in soup.find_all(True):  # True finds all tags\n",
        "      # Get the tag name and text\n",
        "      tag_name = tag.name\n",
        "      tag_text = tag.get_text(strip=True)\n",
        "\n",
        "      # Only add to the dictionary if there's text\n",
        "      if tag_text:\n",
        "          text_dict[tag_name] = tag_contents\n",
        "\n",
        "  return text, text_dict\n"
      ],
      "metadata": {
        "id": "5Y_58zzgh14d"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple initial example with dummy HTML:\n",
        "html_doc = \"\"\"\n",
        "<html><head><title>The Dormouse's story</title></head>\n",
        "<body>\n",
        "\n",
        "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
        "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
        "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
        "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
        "and they lived at the bottom of a well.</p>\n",
        "\n",
        "<p class=\"story\">...</p>\n",
        "\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "# extract content:\n",
        "text, tag_content = extract_text_from_html(html_doc)\n",
        "# print content attached to each contentful html tag:\n",
        "print(\"html tags found: \")\n",
        "for key, value in tag_content.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\nContents of page:\")\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "gFkXjv4N1Wwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, this process removed urls, sas they are stored within HTML `<a>` tags.\n",
        "\n",
        "We can search for content housed in specific tags as well.\n"
      ],
      "metadata": {
        "id": "xaNhUk0SySzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Scrape and extract text\n",
        "\n",
        "We will use [this](https://en.wikisource.org/wiki/Moral_letters_to_Lucilius) wikipedia page, containing the letters written by Seneca.**bold text**"
      ],
      "metadata": {
        "id": "JY_FvL608OMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "def fetch_html(url):\n",
        "    try:\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Raise an exception if the request was unsuccessful\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Return the HTML content\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "WjcPfByp8ALV"
      },
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import page containing links to all of Seneca's letters\n",
        "# get web address\n",
        "src = \"https://en.wikisource.org/wiki/Moral_letters_to_Lucilius\"\n",
        "\n",
        "wiki = requests.get(src).text  # pull html as text\n",
        "# print a small chunk, to see the output:\n",
        "print(textwrap.fill(html_doc, width=70))\n"
      ],
      "metadata": {
        "id": "SaVMRQoTzIoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract plain text from the raw source code"
      ],
      "metadata": {
        "id": "b3w5qA2G9NsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a BeautifulSoup object\n",
        "soup = BeautifulSoup(wiki, 'html.parser')\n",
        "\n",
        "# Extract text, split by full stops:\n",
        "text = soup.get_text(separator='\\n').strip()\n",
        "\n",
        "print(textwrap.fill(text, width=70))"
      ],
      "metadata": {
        "id": "jf8N3cVD8fZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional\n",
        "\n",
        "Try running the subtasks discussed in the previous task, to turn this text dump into input for an NLP pieline."
      ],
      "metadata": {
        "id": "gopYDJmW0HfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pdf files\n",
        "PDF files are a common way to transmit documents. However, they work more like images than a text corpus. So, we will need to extract text from them.\n",
        "\n",
        "For this task, we will use a python library that processed PDFs, and conduct the NLP tasks with NLTK."
      ],
      "metadata": {
        "id": "2sYA-Oab9pz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "FvQc7GTK-YKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Extract names of individuals from an exerpt of a report in PDF format.\n",
        "\n",
        "Consider the names from the [Wiki entry for Adar](https://lotr.fandom.com/wiki/Adar), from the LORT universe.\n",
        "\n",
        "Every entry on the wikipages contains several names, and researching one character could turn into looking up other characters. So, in order to understand what we are getting into, it may be worth it to take stock of all the mentions on a page. :\n",
        "\n",
        "\n",
        "> Adar became known to the peoples of Middle-earth much later in the Second Age, first appearing in a large trench dug by his servant, Magrot, immediately after Arondir's failed attempt to cause a fray and escape. While Arondir is pinned down, having stabbed Magrot in the neck, Lurka orders he be brought to Adar. The \"Lord-father\" then emerges as the Orcs around him bow. He gently tends to the dying Magrot, who had sustained a mortal wound in the Elves' escape attempt, before suddenly ending his suffering with a dagger. As the rest of the Orcs leave, Adar speaks to Arondir, learning the Silvan Elf's birthplace to be in Beleriand. Adar reminisces about his days along the river Sirion, though he evades Arondir's own questions, before releasing Arondir to take a message to the Southlanders taking refuge in the Watchtower of Ostirith: that they may live if they forsake the territory and swear fealty to him. Later, as he watches one of the caged Wargs devouring fresh flesh, Adar is informed by Grugzûk that the Orc Sigil Hilt that they seek is in the watchtower.\n",
        "\n",
        "\n",
        "**What are some of the character names you can spot?**"
      ],
      "metadata": {
        "id": "xfWXj_W9CPKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Optional: run in case of bugs:\n",
        "# !pip install pip==24.0\n",
        "# !pip install textract --no-cache-dir"
      ],
      "metadata": {
        "id": "306473Xe_JVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the PDF\n",
        "import urllib.request\n",
        "import PyPDF2\n",
        "pdf_file = './adar_pdf_example.pdf'\n",
        "pdfreader = PyPDF2.PdfReader(open(pdf_file, 'rb'))\n",
        "# pdfreader = PyPDF2.PdfReader(BytesIO(wFile.read()))"
      ],
      "metadata": {
        "id": "O8WCVoFV_DXO"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting characters from a PDF:"
      ],
      "metadata": {
        "id": "HALeiHlfOnDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting page 2 of the docuemnt\n",
        "pageObj = pdfreader.pages[2]\n",
        "pdf_extract = pageObj.extract_text()\n",
        "print(textwrap.fill(pdf_extract, width=90))"
      ],
      "metadata": {
        "id": "W8o-BSOvBmOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have processed the PDF, let us fix some OCR issues that could impact our search for people. You may notice that the OCR recognition omits a considerable number of spaces.\n",
        "\n",
        "While there are advanced tools for typo detections, because this is fantasy fiction, spell-checkers would also target nouns it doesn't recognize. So, we will stick to regular expressions for cleanup. Since we are only interested in proper nouns, a splitting capitalized words is all we need to do."
      ],
      "metadata": {
        "id": "pHy1FB-uMRS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we re-reun our pipeline for text processing:\n",
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    # We will keep the following characters, and remove the special characters:\n",
        "    text = re.sub(r'[^a-zA-Z\\s.,!\\'?()~@#$%^&*-+=]', '', text)\n",
        "    #Look for capitalized letters in the middle of words, and split the word\n",
        "    # This solves the OCR issue\n",
        "    # text = re.sub(r'(?<!^)(?=[A-Z])', ' ', text)\n",
        "    return text\n",
        "\n",
        "cleaned_corpus = clean_text(pdf_extract)\n",
        "print(textwrap.fill(cleaned_corpus, width=90))"
      ],
      "metadata": {
        "id": "hT_dOPX144Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilizing the NLTK pipeline\n",
        "\n",
        "Next, we will run the NLTK pipeline to extract names entities. The `ne_chunk`[link text](https:// [link text](https://)) function adds a lot of information to tokens."
      ],
      "metadata": {
        "id": "rEUx45VpOxIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "from nltk.tree import Tree\n",
        "\n",
        "import re\n"
      ],
      "metadata": {
        "id": "YtX2egm858h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the cleaned corpus\n",
        "def clean_nltk(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  tagged = pos_tag(tokens)\n",
        "  named_entities = ne_chunk(tagged)\n",
        "  return named_entities\n",
        "\n",
        "# Parse and tag each sentence, and extract named entities:\n",
        "nltk_results = clean_nltk(cleaned_corpus)\n",
        "\n",
        "print(nltk_results)\n"
      ],
      "metadata": {
        "id": "7B3FGfSB_V7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will only need the contents of the Tree item for each named entity, which provides all the words\n",
        "# clustered for each NE label, along with their POS tags\n",
        "# Learn more about this module here: https://www.nltk.org/api/nltk.tree.tree.html#nltk.tree.tree.Tree\n",
        "named_entities = [ne for ne in nltk_results if type(ne) == Tree]\n",
        "print(*(ne for ne in named_entities[:20]), sep='\\n')\n",
        "\n",
        "#  Try and see what is excluded from this search.\n",
        "# You will see POS tags, but no NE labels:\n",
        "# not_named_entities = [nne for nne in nltk_results if type(nne) != Tree]\n",
        "# print(*(ne for ne in not_named_entities[:20]), sep='\\n')"
      ],
      "metadata": {
        "id": "cK3Yh7S4I79t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Person names from the list of NERs"
      ],
      "metadata": {
        "id": "1Driv5T-O_oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will notice that the label \"PERSON\" is the one relevant to our search. Because this is a fantasy universe, some people have also been labelled as \"GBE\", or geographical entities. For the purposes of this exercise, we will only extract named entities with the label \"PERSON\"\n",
        "\n",
        "Try this code with some real-world text and compare the results!"
      ],
      "metadata": {
        "id": "VM9w2iWDQc2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we will only keep the Named Entities with the label \"PERSON\"\n",
        "for ne in named_entities:\n",
        "    if ne.label() == 'PERSON':\n",
        "        # join all the nouns housed in the label:\n",
        "        name = ''\n",
        "        for leaf in ne.leaves():\n",
        "            name += leaf[0] + ' '\n",
        "        print ('Type: ', ne.label(), 'Name: ', name)"
      ],
      "metadata": {
        "id": "5m94ZCA6dlxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final thoughts\n",
        "\n",
        "In this task, we worked on some simple text pre-processing steps for text extracted from different sources. I used the NLTK took for this purpose. We encountered different kinds of noise for different sources, and needed different approaches and additional steps, if the complexity of the task was higher. When we moved to a fantsy universe, our model did a fairly good job, but faced issues if the text pre-processing had errors (such as incorrect POS tagging). The purpose of showing such a complex task was to examine how important text pre-processing tools are for other downstream tasks.\n",
        "\n",
        "As this is a GIGO situation, set ups with language models trained on more data (potentially Spacy or even an LLM) coudl do an even better job, or be able to handle noisy data. We demonstrated some lightweight setups for fast processing. Try out this code with other pre-processing tools and tell us about your experience!"
      ],
      "metadata": {
        "id": "kAokU70KfHqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sources and References\n",
        "\n",
        "- https://www.geeksforgeeks.org/text-preprocessing-for-nlp-tasks/\n",
        "- https://www.xbyte.io/how-to-do-web-scraping-and-pre-processing-for-nlp-using-python/\n",
        "- https://ydv-poonam.medium.com/how-to-extract-text-from-a-pdf-nlp-b6409422cfd2\n",
        "- https://unbiased-coder.com/extract-names-python-nltk/\n",
        "- https://towardsdatascience.com/elegant-text-pre-processing-with-nltk-in-sklearn-pipeline-d6fe18b91eb8"
      ],
      "metadata": {
        "id": "PDEUMizW96Fh"
      }
    }
  ]
}