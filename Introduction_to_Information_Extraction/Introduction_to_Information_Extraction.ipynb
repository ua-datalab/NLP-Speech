{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOpdmqkkiP4gJlf3kMoTALl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ua-datalab/NLP-Speech/blob/main/Introduction_to_Information_Extraction/Introduction_to_Information_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1> Introduction to Information Extraction </h1> </center>\n",
        "\n",
        "\n",
        "\n",
        "![](https://www.tex-ai.com/wp-content/uploads/2020/10/NLP-techniques-for-information-extraction.jpg)"
      ],
      "metadata": {
        "id": "Y1Qe19Kv212z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Housekeeping\n",
        "1. Check that the recording is on\n",
        "2. Check audio and screenshare\n",
        "3. Share link to notebook in chat\n",
        "4. Light mode and readable font size"
      ],
      "metadata": {
        "id": "h00WlRlEOwuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is information extraction?\n",
        "\n",
        "Information extraction (IE) involves converting *unstructured or semi-structured information* from *machine readable documents* into *structured knowledge*, which can be queried to *automatically access* specific information *at scale*.\n",
        "\n",
        "Broadly, it is the culmintion of NLP, dataset creation, and  search and retrieval.\n",
        "\n",
        "It is a move from data processing, to incorporating knowledge into tasks.\n",
        "\n",
        "### Terminology\n",
        "- *Unstructured or semi-structured information*\n",
        "- *Machine readable documents*\n",
        "- *Structured knowledge*\n",
        "- *Automatically access*\n",
        "- *At scale*"
      ],
      "metadata": {
        "id": "_ntT6SjC3BNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing the World Wide Web- an infinite knowledge source\n",
        "- Millions of contributors, and agregate agreement on correctness of information\n",
        "- Easy to train algorithms to access human-created information\n",
        "- Large group of people who can agree on how entities are related to each other\n",
        "\n",
        "### Shallow vs Advanced Information extraction\n",
        "\n",
        "- Reguar Expressions\n",
        "  - Matching patterns (such as capitalized letters) or digits (money)\n",
        "- Matching keywords against registries\n",
        "  - Names\n",
        "  - Geographical entities\n",
        "  - Currency\n",
        "\n",
        "With the advent of NLP, we are able to add a lot of information to text, and with LLMs, we can add embeddings that better model relationships between words in a document. So, we can move from extracting expected information to processing documents better to search for information better."
      ],
      "metadata": {
        "id": "-zazPxFMPLEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What can we extract?\n",
        "\n",
        "### Named entity recognition (NER) or identification\n",
        "- Aims at finding real-world objects in texts.\n",
        "- Classifies them into predefined categories such as names of persons, organizations, locations, temporal expressions, products, etc.\n",
        "\n",
        "\n",
        "Question: is this an example best solved by shallow or deep IE?\n",
        "\n",
        "### Quantities and monetary values\n",
        "- Currency\n",
        "- Stocks\n",
        "- Number\n",
        "\n",
        "### Entity Disambiguation and Term Evolution\n",
        "- Same name may point to two different entities\n",
        "- An entity may have a new name\n",
        "\n",
        "Question: How does one differenciate information about the Bush administrations of two different presidents?\n",
        "\n",
        "Question: How do we connect news about Twitter from 2010-2024?\n",
        "\n",
        "### Entity classes\n",
        "- categories assigned to an entity\n",
        "- Determines its relationship with other entities\n",
        "\n",
        "A given entity can belong to more than one class.\n",
        "\n",
        "Question: what are some classes Mark Zuckerburg belongs to?\n",
        "\n",
        "### Entity Relations\n",
        "- Meaning building involves connecting concepts\n",
        "- IE includes extracting those connections"
      ],
      "metadata": {
        "id": "8s_CT6R-PcfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Creating the components of a knowledge base\n",
        "\n",
        "Information extraction is thus motivated to enrich a knowledge base that can be imporved and addded to. This happens by:\n",
        "\n",
        " - Selecting data sources (newspaper articles, reddit posts, question-answer datasets)\n",
        " - Extracting entities, classes, and relationships in the dataset\n",
        " - Consistntly integrating and linking new information in the right place in an existing knowledge base"
      ],
      "metadata": {
        "id": "uDTLDlm9a2L5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How do we carry out information extraction?\n",
        "\n",
        "- Data processing:\n",
        "  - Tokenization\n",
        "  - Tagging\n",
        "  - Stop word removal\n",
        "  - Dependency parsing\n",
        "\n",
        "- Finding a language model trained to find relations and entities\n",
        "\n",
        "- Processing the output"
      ],
      "metadata": {
        "id": "b2LuVSTk3XJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some more concepts\n",
        "### Precision\n",
        "### Recall\n",
        "### TF/IDF\n",
        "\n"
      ],
      "metadata": {
        "id": "FlXyRVPTUvCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why do we need this?\n",
        "\n",
        "Essential for downstream tasks, such as text summarization, text classification and"
      ],
      "metadata": {
        "id": "O1RPvP_1XnFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working examples of information extraction\n",
        "\n",
        "1. Information Collection\n",
        "2. Process Data\n",
        "3. Choosing the Right Model\n",
        "4. Evaluation of the Model\n",
        "5. Deploying Model in Production"
      ],
      "metadata": {
        "id": "p5infNPp3oMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entity extraction with SpaCy\n",
        "\n",
        "Source: https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/"
      ],
      "metadata": {
        "id": "4pIHBiAG7bF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install spacy\n",
        "! python -m spacy download en_core_web_sm\n",
        "# import and load the English language model for vocabluary, syntax & entities\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "#for visualization of Entity detection importing displacy from spacy:\n",
        "\n",
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "_Wt6rXSuYgy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "nytimes= nlp(u\"\"\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\n",
        "\n",
        "At least 285 people have contracted measles in the city since September, mostly in Brooklynâ€™s Williamsburg neighborhood. The order covers four Zip codes there, Mayor Bill de Blasio (D) said Tuesday.\n",
        "\n",
        "The mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old. Anyone who resists could be fined up to $1,000.\"\"\")\n",
        "\n",
        "entities=[(i, i.label_, i.label) for i in nytimes.ents]\n",
        "entities"
      ],
      "metadata": {
        "id": "-2BW1M9ZYfmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(nytimes, style = \"ent\",jupyter = True)\n"
      ],
      "metadata": {
        "id": "9wCiWOtPZD4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query tags:\n",
        "from spacy import explain\n",
        "spacy.explain(\"GPE\")"
      ],
      "metadata": {
        "id": "GTMdObXKZUtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple LLM-based relation extractor\n",
        "\n",
        "This example uses Rebel, a simple relation extractor in the form of a triplet (Entity1, Relation, Entity2)\n",
        "\n",
        "Model source: https://huggingface.co/Babelscape/rebel-large"
      ],
      "metadata": {
        "id": "Je8K2NWf72n-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEfeRAeW21L1"
      },
      "outputs": [],
      "source": [
        "#Load model and format for creating the triplets:\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\n",
        "gen_kwargs = {\n",
        "    \"max_length\": 256,\n",
        "    \"length_penalty\": 0,\n",
        "    \"num_beams\": 3,\n",
        "    \"num_return_sequences\": 3,\n",
        "}\n",
        "\n",
        "# Extract triplets from text:\n",
        "def extract_triplets(text):\n",
        "    triplets = []\n",
        "    relation, subject, relation, object_ = '', '', '', ''\n",
        "    text = text.strip()\n",
        "    current = 'x'\n",
        "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
        "        if token == \"<triplet>\":\n",
        "            current = 't'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "                relation = ''\n",
        "            subject = ''\n",
        "        elif token == \"<subj>\":\n",
        "            current = 's'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "            object_ = ''\n",
        "        elif token == \"<obj>\":\n",
        "            current = 'o'\n",
        "            relation = ''\n",
        "        else:\n",
        "            if current == 't':\n",
        "                subject += ' ' + token\n",
        "            elif current == 's':\n",
        "                object_ += ' ' + token\n",
        "            elif current == 'o':\n",
        "                relation += ' ' + token\n",
        "    if subject != '' and relation != '' and object_ != '':\n",
        "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "    return triplets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function for processing a text corpus:\n",
        "def extract_triplets(text):\n",
        "  # Tokenizer text\n",
        "  model_inputs = tokenizer(text, max_length=1000, padding=True, truncation=True, return_tensors = 'pt')\n",
        "\n",
        "  # Generate\n",
        "  generated_tokens = model.generate(\n",
        "      model_inputs[\"input_ids\"].to(model.device),\n",
        "      attention_mask=model_inputs[\"attention_mask\"].to(model.device),\n",
        "      **gen_kwargs,\n",
        "  )\n",
        "\n",
        "  # Extract text\n",
        "  decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
        "\n",
        "  # Print triplets:\n",
        "  for idx, sentence in enumerate(decoded_preds):\n",
        "      print(f'Prediction triplets {idx}')\n",
        "      [print(f\"Entity1: {item['head']}\\n Relation:{item['type']}\\n Entity2:{item['tail']}\") for item in extract_triplets(sentence)]"
      ],
      "metadata": {
        "id": "SvXX8QfAp2Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text to extract triplets from\n",
        "text = 'Batman, created by the artist Bob Kane and writer Bill Finger, is a superhero who appears in American comic books published by DC Comics \\\n",
        "        and debuted in the 27th issue of the comic book Detective Comics on March 30, 1939.\\\n",
        "        In the DC Universe, Batman is the alias of Bruce Wayne, a wealthy American playboy, \\\n",
        "        philanthropist, and industrialist who resides in Gotham City.\\\n",
        "        His origin story features him swearing vengeance against criminals after witnessing the murder of his parents,\\\n",
        "        Thomas and Martha.'\n"
      ],
      "metadata": {
        "id": "-KysOyHJm1VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sent in enumerate(text.split('.')):\n",
        "  print(f'Sentence being processed: {i}')\n",
        "  print_triplets(sent)"
      ],
      "metadata": {
        "id": "aavlvs62m2D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic resume scraping with LLMs\n",
        "\n",
        "Source: https://huggingface.co/foduucom/resume-extractor\n",
        "\n",
        "This example uses a LLM to extract information from a resume in the PDF format.\n",
        "\n",
        "A sobering example of how resumes get auto-rejected."
      ],
      "metadata": {
        "id": "wZX9RZh27P6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install and download ollama with dependencies\n",
        "! sudo apt-get install -y pciutils\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "!pip install ollama langchain_community pdfminer.six\n",
        "\n",
        "# import necessary python libraries\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n"
      ],
      "metadata": {
        "id": "NKNXtLGhW9Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start Ollama\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ],
      "metadata": {
        "id": "IfAukBYRAL4y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run embedding model\n",
        "!ollama run llama3"
      ],
      "metadata": {
        "id": "wCfNgPzWAMq6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "\n",
        "json_content = \"\"\"{{\n",
        "    \"name\": \"\",\n",
        "    \"email\" : \"\",\n",
        "    \"phone_1\": \"\",\n",
        "    \"phone_2\": \"\",\n",
        "    \"address\": \"\",\n",
        "    \"city\": \"\",\n",
        "    \"linkedin\": \"\",\n",
        "    \"professional_experience_in_years\": \"\",\n",
        "    \"highest_education\": \"\",\n",
        "    \"is_fresher\": \"yes/no\",\n",
        "    \"is_student\": \"yes/no\",\n",
        "    \"skills\": [\"\",\"\"],\n",
        "    \"applied_for_profile\": \"\",\n",
        "    \"education\": [\n",
        "        {{\n",
        "            \"institute_name\": \"\",\n",
        "            \"year_of_passing\": \"\",\n",
        "            \"score\": \"\"\n",
        "        }},\n",
        "        {{\n",
        "            \"institute_name\": \"\",\n",
        "            \"year_of_passing\": \"\",\n",
        "            \"score\": \"\"\n",
        "        }}\n",
        "    ],\n",
        "    \"professional_experience\": [\n",
        "        {{\n",
        "            \"organisation_name\": \"\",\n",
        "            \"duration\": \"\",\n",
        "            \"profile\": \"\"\n",
        "        }},\n",
        "        {{\n",
        "            \"organisation_name\": \"\",\n",
        "            \"duration\": \"\",\n",
        "            \"profile\": \"\"\n",
        "        }}\n",
        "    ]\n",
        "}}\"\"\"\n",
        "\n",
        "\n",
        "class InputData:\n",
        "    def input_data(text):\n",
        "\n",
        "        input = f\"\"\"Extract relevant information from the following resume text and fill the provided JSON template.\n",
        "                    Ensure all keys in the template are present in the output,\n",
        "                    even if the value is empty or unknown.\n",
        "                    If a specific piece of information is not found in the text, use 'Not provided' as the value.\n",
        "\n",
        "        Resume text:\n",
        "        {text}\n",
        "\n",
        "        JSON template:\n",
        "        {json_content}\n",
        "\n",
        "        Instructions:\n",
        "        1. Carefully analyse the resume text.\n",
        "        2. Extract relevant information for each field in the JSON template.\n",
        "        3. If a piece of information is not explicitly stated, make a reasonable inference based on the context.\n",
        "        4. Ensure all keys from the template are present in the output JSON.\n",
        "        5. Format the output as a valid JSON string.\n",
        "\n",
        "        Output the filled JSON template only, without any additional text or explanations.\"\"\"\n",
        "\n",
        "        return input\n",
        "\n",
        "    def llm():\n",
        "        llm = Ollama(model=\"llama3\")\n",
        "        return llm"
      ],
      "metadata": {
        "id": "S6Sz0e4vLN1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "import sys\n",
        "sys.path.append(\"/content/resume-extractor/\")\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)\n",
        "\n",
        "text = extract_text_from_pdf(r'/content/anti-cv.pdf')\n",
        "\n",
        "llm = input.llm()\n",
        "data = llm.invoke(input.input_data(text))\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2RFadcllHc0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "- https://nanonets.com/blog/information-extraction/\n",
        "- https://www.geeksforgeeks.org/information-extraction-in-nlp/\n",
        "- \"Introduction to Information Extraction: Basic Notions and Current Trends\"\n"
      ],
      "metadata": {
        "id": "zcvBYzTW3uH-"
      }
    }
  ]
}