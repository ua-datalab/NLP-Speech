{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eqWKxcSXenoc"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMckAmHjNk4gqvhOmg76K9H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ua-datalab/NLP-Speech/blob/main/Introduction_to_Speech_Technology/Introduction_to_Speech_Technology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<h1><center>Introduction to Speech Technology</center></h1>"
      ],
      "metadata": {
        "id": "eqWKxcSXenoc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://www.csail.mit.edu/sites/default/files/styles/primary_image/public/2017-12/ASR1_0.png)"
      ],
      "metadata": {
        "id": "MYTn6sEEDU1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Housekeeping\n",
        "1. Check that the recording is on\n",
        "2. Check audio and screenshare\n",
        "3. Share link to notebook in chat\n",
        "4. Light mode and readable font size"
      ],
      "metadata": {
        "id": "eRZoNyjDCLDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "Speech technology includes processes and pipelines that record, encode, process and decode spoken human langauge for processes such as voice recognition, transcription, biometrics, speech synthesis and machine translation.\n",
        "\n",
        "Speech processing, in association with NLP processes human language at multiple linguistic levels: from the phonetic and phonological, to the syntactic and cognitive.\n",
        "\n",
        "With advancements in voice capturing interfaces, GPUs, and deep learning, speech technology can now process human conversations nealy in real-time, making communication with systems more accessible and natural.\n"
      ],
      "metadata": {
        "id": "X-hMhWIFxHV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Popular Use Cases\n",
        "\n",
        "## Speaker diarization and biometrics\n",
        "Voice recognition assists in splitting audio with multiple speakers. It is also a tool for  voice identification\n",
        "\n",
        "## Transcription and captioning, voice typing\n",
        "With a language model, ASR can be used to turn the speech signal into text with a predictive element, often with very little lag.\n",
        "\n",
        "## Sound source identification\n",
        "We can identify speech, background noise, bird calls, music by processing sound files and looking for sounds of different nature\n",
        "\n",
        "## Speech synthesis\n",
        "New sounds and voices can be generated with user-specified parameters\n",
        "\n",
        "## Voice assistants and voice-activated technology\n",
        "Google, Amazon and other devices can control devices and perform tasks with a activation command and prompts\n",
        "\n",
        "## Emotion, sentiment and voice recognition\n",
        "By combining multiple linguistic levels using the speech mannerisms and the  contents of the dialog, we can do a much better job predicting/perceiving emotional state, sarcasm, and speaker intent.\n",
        "\n",
        "## Machine translation and language detection\n",
        "Every language has its own phonology, phonotactics, and linguistic elements. Speech offers a great way to use speech tchnology and NLP to differentiate langauges, and translate between them\n",
        "\n",
        "How many of these have you encountered this week?"
      ],
      "metadata": {
        "id": "7EbSvo3yxPpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/ua-datalab/NLP-Speech/main/Introduction_to_Speech_Technology/asr_pipeline.png)"
      ],
      "metadata": {
        "id": "cYnQkJoqPZCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speech Processing: terminology"
      ],
      "metadata": {
        "id": "ha-NWrtR78VH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Speech Recognition (ASR - Automatic Speech Recognition):\n",
        "Converting spoken language into text by recognizing speech patterns and interpreting them into written language.\n",
        "Ex. virtual assistants trained to understand user commands\n",
        "2. Natural Language Processing (NLP): Enabling machines to understand, interpret, and generate human language\n",
        "3. Speech Synthesis: artificial production of human speech by converting text into sound using algorithms and models.\n",
        "4. Text-to-Speech (TTS): The process of taking written text as prompts, and outputting spoken words. Ex.: every Twitch streamer trying to read donation information while gaming.\n",
        "5. Voice User Interface (VUI): The interface that allows users to use voice commands as input.\n",
        "7. Acoustic Model: A (usually) probabalistic or deep neural network model trained to represent the relationship between linguistic units of speech and their audio signals\n",
        "8. Language Model: A statistical model that predict the likelihood of word sequences based on prior language patterns.\n",
        "\n",
        "10. Wake Word\n",
        "Definition: A specific word or phrase used to activate a voice-controlled system, alerting it to start listening for further commands.\n",
        "Use Case: In devices like Alexa, \"Alexa\" is the wake word that prompts the system to start processing voice commands.\n",
        "Bonus Terms (Optional):\n",
        "Dialog Manager: The system responsible for controlling the flow of conversation in a voice interaction, ensuring that responses are contextually appropriate.\n",
        "Speech Corpus: A large collection of recorded speech data used to train speech recognition models and improve their accuracy.\n",
        "Understanding and defining these terms will give your audience a solid foundation to grasp the technical aspects of speech technology."
      ],
      "metadata": {
        "id": "W1yhle0jPdpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# History of Speech Techology\n",
        "![](https://miro.medium.com/v2/resize:fit:3456/format:webp/1*LtuOUuCSwAKh8Um7_NCHmA.png)"
      ],
      "metadata": {
        "id": "F4DrvIImOY7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The world's shorter phonetics + phonology primer\n",
        "\n"
      ],
      "metadata": {
        "id": "390Yyz9zfEa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sound propagation\n",
        "- Sound source\n",
        "- [Longitudinal waves, compressions and rarefactions](https://www.physicsclassroom.com/class/sound/Lesson-1/Sound-as-a-Longitudinal-Wave)\n",
        "- Medium\n",
        "- Resonance tube"
      ],
      "metadata": {
        "id": "LVTFjDzC3_Zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phonetics + Physics concepts worth understanding better:\n",
        "- Amplitude: how much energy?\n",
        "- Time: duration of the wave's propogation\n",
        "- Frequency: how often does the wave occilate per unit time?\n",
        "- Simple vs Complex waves: wave with only 1 frequency component, wave with only multiple frequency components\n",
        "- Waveform: Plot of amplitude of the sound wave over time\n",
        "- Spectrum: Plot of magnitude of each frequency component, or number of occilations of each component per second\n",
        "- Spectrogram: Plot of changes in frequency components of the sound change over time\n",
        "- Fourier Transform: A mathematical function that represents a wave in the frequency domain\n",
        "- Formants: In a speech wave, resonance in the oral cavity amplifies some components of the wave, and dampens others. This causes a concentration of energy (higher amplitude) around a particular frequency in the speech wave. For human speech generates several formants, each at a different frequency ( ~one in every 1000Hz band for an average male voice, 1100Hz for average female voice).\n",
        "- MFCCs: A the \"short-term\" power spectrum of sound, often captured every 5-20 miliseconds. They are collected by applying a series of transformations (like a mel-frequency scale) to extract a representation that is closer to that of human auditory processing."
      ],
      "metadata": {
        "id": "Ww3ptXbl7re3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "# Generate time array\n",
        "t = np.linspace(0, 1.0, int(1000 * 1.0), endpoint=False)\n",
        "\n",
        "# Generate a tone (sine wave) at 5000Hz\n",
        "tone = np.sin(2 * np.pi * 5000 * t)\n",
        "Audio(tone, rate=1000)"
      ],
      "metadata": {
        "id": "WTgjwAIKlBhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.fft import fft, fftfreq\n",
        "from scipy.io.wavfile import write\n",
        "from scipy.signal import spectrogram\n",
        "from google.colab import files\n",
        "\n",
        "def visualize_tone_with_spectrogram(frequency, duration=1.0, sampling_rate=1000, filename='tone.wav'):\n",
        "    # Generate time array\n",
        "    t = np.linspace(0, duration, int(sampling_rate * duration), endpoint=False)\n",
        "\n",
        "    # Generate the tone (sine wave)\n",
        "    tone = np.sin(2 * np.pi * frequency * t)\n",
        "    Audio(tone, rate=1000)\n",
        "\n",
        "\n",
        "    # Normalize tone to the range of int16 for saving as a .wav file\n",
        "\n",
        "    # Perform the Fourier Transform to get the frequency spectrum\n",
        "    N = len(tone)\n",
        "    yf = fft(tone)\n",
        "    xf = fftfreq(N, 1 / sampling_rate)\n",
        "\n",
        "    # Only take the positive part of the frequency spectrum\n",
        "    xf = xf[:N // 2]\n",
        "    yf = np.abs(yf[:N // 2])\n",
        "\n",
        "    # Calculate the spectrogram\n",
        "    f, t_spec, Sxx = spectrogram(tone, fs=sampling_rate)\n",
        "\n",
        "    # Plot the waveform, spectrum, and spectrogram\n",
        "    plt.figure(figsize=(12, 9))\n",
        "\n",
        "    # Time domain (waveform)\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(t, tone)\n",
        "    plt.title(\"Waveform: Tone in Time Domain: Changes in energy over time\")\n",
        "    plt.xlabel(\"Time [s]\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "\n",
        "    # Frequency domain (spectrum)\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(xf, yf)\n",
        "    plt.title(\"Spectrum: Tone in Frequency Domain: number of cycles per second\")\n",
        "    plt.xlabel(\"Frequency [Hz]\")\n",
        "    plt.ylabel(\"Magnitude\")\n",
        "\n",
        "    # Spectrogram\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.pcolormesh(t_spec, f, 10 * np.log10(Sxx), shading='gouraud')\n",
        "    plt.title(\"Spectrogram: energy changes of each component (in this case, 1) over time\")\n",
        "    plt.ylabel(\"Frequency [Hz]\")\n",
        "    plt.xlabel(\"Time [s]\")\n",
        "    plt.colorbar(label='Intensity [dB]')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Save the tone as a .wav file\n",
        "    # write(filename, sampling_rate, np.int16(tone * 32767))\n",
        "    # print(f\"Audio file saved as {filename}.\")\n",
        "\n",
        "    # Provide download link for the audio file\n",
        "    # files.download(filename)\n",
        "\n",
        "# Example usage\n",
        "visualize_tone_with_spectrogram(frequency=5, duration=2.0, sampling_rate=1000, filename='tone.wav')\n"
      ],
      "metadata": {
        "id": "7ukF7DqHrIeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.fft import fft, fftfreq\n",
        "from scipy.io.wavfile import write\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "def generate_vowel_sound(duration=1.0, sampling_rate=1000):\n",
        "    \"\"\"\n",
        "    Generate a vowel-like sound with specific formants.\n",
        "    The formants are represented as peaks in the frequency domain.\n",
        "    \"\"\"\n",
        "    t = np.linspace(0, duration, int(sampling_rate * duration), endpoint=False)\n",
        "\n",
        "    # Frequencies for formants (F1, F2, F3)\n",
        "    f1 = 700  # F1\n",
        "    f2 = 1200  # F2\n",
        "    f3 = 2800  # F3\n",
        "\n",
        "    # Generate the vowel sound by adding formants\n",
        "    wave = (0.5 * np.sin(2 * np.pi * f1 * t) +\n",
        "            0.25 * np.sin(2 * np.pi * f2 * t) +\n",
        "            0.125 * np.sin(2 * np.pi * f3 * t))\n",
        "\n",
        "    # Normalize the wave\n",
        "    wave = wave / np.max(np.abs(wave))\n",
        "\n",
        "    return t, wave\n",
        "\n",
        "def generate_noise_sound(duration=1.0, sampling_rate=1000):\n",
        "    \"\"\"\n",
        "    Generate a noise sound without any formants.\n",
        "    \"\"\"\n",
        "    t = np.linspace(0, duration, int(sampling_rate * duration), endpoint=False)\n",
        "\n",
        "    # Generate white noise\n",
        "    wave = np.random.uniform(-1, 1, t.shape)\n",
        "\n",
        "    # Normalize the wave\n",
        "    wave = wave / np.max(np.abs(wave))\n",
        "\n",
        "    return t, wave\n",
        "\n",
        "def compute_mfcc(wave, sampling_rate=1000, n_mfcc=13, n_fft=256):\n",
        "    \"\"\"\n",
        "    Compute MFCCs from the given wave.\n",
        "    \"\"\"\n",
        "    mfccs = librosa.feature.mfcc(y=wave, sr=sampling_rate, n_mfcc=n_mfcc, n_fft=n_fft)\n",
        "    return mfccs\n",
        "\n",
        "def plot_mfccs(wave, sampling_rate=1000, title = \"MFCCs\", n_mfcc=13, n_fft=256):\n",
        "    \"\"\"\n",
        "    Compute and plot MFCCs for the given wave.\n",
        "    \"\"\"\n",
        "    # Compute MFCCs\n",
        "    mfccs = compute_mfcc(wave, sampling_rate, n_mfcc, n_fft)\n",
        "\n",
        "    # Plot MFCCs\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(mfccs, sr=sampling_rate, x_axis='time')\n",
        "    plt.title(title)\n",
        "    plt.colorbar(label='MFCC Coefficients')\n",
        "    plt.xlabel('Time [s]')\n",
        "    plt.ylabel('MFCC Coefficient Index')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_waveform_and_spectrum(t, wave, sampling_rate=1000, title=\"Waveform and Spectrum\"):\n",
        "    \"\"\"\n",
        "    Plot waveform and frequency spectrum for the given wave.\n",
        "    \"\"\"\n",
        "    # Perform Fourier Transform\n",
        "    N = len(wave)\n",
        "    yf = fft(wave)\n",
        "    xf = fftfreq(N, 1 / sampling_rate)\n",
        "\n",
        "    # Take the positive half of the spectrum\n",
        "    xf = xf[:N // 2]\n",
        "    yf = np.abs(yf[:N // 2])\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Waveform\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(t, wave)\n",
        "    plt.title(f\"{title} - Waveform\")\n",
        "    plt.xlabel(\"Time [s]\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "\n",
        "    # Frequency spectrum\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(xf, yf)\n",
        "    plt.title(f\"{title} - Frequency Spectrum\")\n",
        "    plt.xlabel(\"Frequency [Hz]\")\n",
        "    plt.ylabel(\"Magnitude\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QhNfgnToyjkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sounds\n",
        "duration = 1.0  # Full duration of 1 second\n",
        "sampling_rate = 1000\n",
        "\n",
        "# Generate vowel sound with formants\n",
        "t_vowel, wave_vowel = generate_vowel_sound(duration, sampling_rate)\n",
        "write(\"vowel_sound.wav\", sampling_rate, (wave_vowel * 32767).astype(np.int16))  # Save as .wav file\n",
        "\n",
        "# Generate noise sound without formants\n",
        "t_noise, wave_noise = generate_noise_sound(duration, sampling_rate)\n",
        "write(\"noise_sound.wav\", sampling_rate, (wave_noise * 32767).astype(np.int16))  # Save as .wav file\n",
        "\n"
      ],
      "metadata": {
        "id": "tJX91KJFl8Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vowel = read_audio(\"/content/vowel_sound.wav\").squeeze()\n",
        "Audio(vowel, rate=sampling_rate)"
      ],
      "metadata": {
        "id": "teSi0BG6mJRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise = read_audio(\"/content/noise_sound.wav\").squeeze()\n",
        "Audio(noise, rate=sampling_rate)"
      ],
      "metadata": {
        "id": "RavrEEJWm2lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot waveform and spectrum for the vowel sound (with formants)\n",
        "plot_waveform_and_spectrum(t_vowel[:500], wave_vowel[:500], sampling_rate, title=\"Vowel Sound\")\n",
        "\n",
        "# Plot waveform and spectrum for the noise sound (without formants)\n",
        "plot_waveform_and_spectrum(t_noise[:500], wave_noise[:500], sampling_rate, title=\"Noise Sound\")\n",
        "\n",
        "# Plot MFCCs for the vowel sound (full duration)\n",
        "plot_mfccs(wave_vowel, sampling_rate, title = \"MFCCs for vowel-like sound\")\n",
        "\n",
        "# Plot MFCCs for the noise sound (full duration)\n",
        "plot_mfccs(wave_noise, sampling_rate, \"MFCCs for noise\")"
      ],
      "metadata": {
        "id": "Riq9xVsimGUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phonology/ Phonemic inventory, phonotactics, prosody\n",
        "\n",
        "Now that we have understood how sound is produced, let us look at some specific components of human speech.\n",
        "- Phones, phonemes: smallest unit of sound distinct from others, which can build bigger linguistic units.\n",
        "- Prosody: the rhythm, stress, and intonation of speech, which can affect meaning and emotional tone. Prosody helps speech sound emotive, natural and varies by language.\n",
        "- Phonology: Permissible and prohibitted sound patterns of a given language, building its inventory of possible sounds.\n",
        "- Phonotactics: Srudy of combinations and arrangements of sounds (phonemes) in a particular language, governing the permissible structures of syllables and words.\n",
        "\n",
        "### Constraints:\n",
        "- Human speech is versatile. But with our articulatory system, we can only producea limited set of sounds, which overlap with each other\n",
        "Below is a 'vocabulary' containing all phones we can create with our articulatory system.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/8f/IPA_chart_2020.svg\" width=\"500\"/>\n",
        "\n",
        "- For speech to actually be useful, sounds have to be arranged in such a way that they can be differentiated, or else words will sound the same.\n",
        "- Human language is purpose-driven, works with all linguistic levels (unless you are really angry, sad or heard a great joke).\n",
        "\n",
        "\n",
        "*So speech processing is NOT sound processing, but language processing*"
      ],
      "metadata": {
        "id": "3Hfwoao_8VGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human Articulatory, Acoustic and Perception Systems:\n",
        "\n",
        "### Articulation AKA the \"ling\" in linguistics\n",
        "- Vocal folds (source)\n",
        "- Oral and nasal cavities (resonance tubes)\n",
        "- Articulators: tongue, teeth, palettes, velum\n",
        "\n",
        "[The Pink Trombone Demo](https://dood.al/pinktrombone/)\n",
        "\n",
        "### Acoustics\n",
        "- Ear\n",
        "- Ear membrane\n",
        "[Interactive ear demo](https://www.amplifon.com/uk/interactive-ear/index.html)\n",
        "\n",
        "### Perception\n",
        "Taking a continuous sound stream, identifying speaker characteristics, splitting it down to phones, and processing speech along with visual and linguistic processing.\n"
      ],
      "metadata": {
        "id": "tksQArDi6BKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "video_id = \"KiuO_Z2_AD4\"\n",
        "video_url = f\"https://www.youtube.com/embed/{video_id}\"\n",
        "\n",
        "# Create an HTML iframe to embed the video\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\" src=\"{video_url}\" frameborder=\"0\" allowfullscreen></iframe>\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "2HidsBQ8_9jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Things I can't cover today, but want to\n",
        "\n",
        "Converting a speech signal into something that can be stored, transmitted and made machine-readable requires some additional steps and terminology:\n",
        "\n",
        "- Analog vs digital signals\n",
        "- Digital Signal Processing\n",
        "- Digitization\n",
        "- Sampling (and sampling rate)\n",
        "- Quantization\n",
        "- Setting the bitrate\n",
        "- Compression\n",
        "\n",
        "At the end of this process, we have sound in a format, that can be shared, stored, and processed in the pipeline.\n",
        "\n",
        "The sound files can be in a whole host of formats (`.mp3, .flac, .wav, mp4a`, etc.). Command line tools such as `ffmpeg` allow us to transform sound files across formats, and change their parameters so that they can be fed into different pipelines easily."
      ],
      "metadata": {
        "id": "wFLpHwVGD3yG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now that we have covered the human side of speech technology, let us move to some terms on the machine side of the equation.**"
      ],
      "metadata": {
        "id": "Xh-0_ZeK7xb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demos"
      ],
      "metadata": {
        "id": "KWvOg8fiEeu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text to Speech with Coqui\n",
        "\n",
        "[Code Source](https://github.com/ua-datalab/Generative-AI/blob/enoriega/langchain/Notebooks/HuggingFace_Multi_Modal.ipynb)"
      ],
      "metadata": {
        "id": "fRSQpJ5IekD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TTS\n",
        "!pip install torch\n"
      ],
      "metadata": {
        "id": "2M66Ab33KAF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import speech to text models and an interface using Coqui\n",
        "import torch\n",
        "from pprint import pprint\n",
        "from TTS.api import TTS\n",
        "\n",
        "# Get device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# List available 🐸TTS models\n",
        "pprint(TTS().list_models())\n",
        "\n"
      ],
      "metadata": {
        "id": "_fgi3CGYF4eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a multilingual model\n",
        "tts = TTS(\"tts_models/multilingual/multi-dataset/your_tts\")"
      ],
      "metadata": {
        "id": "sWYVo5JPKN96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O sample.m4a https://github.com/ua-datalab/Generative-AI/raw/refs/heads/enoriega/langchain/Notebooks/sample.m4a\n",
        "\n",
        "tts.tts_to_file(\"This is voice cloning.\", speaker_wav=\"sample.m4a\", language=\"en\", file_path=\"output.wav\")\n",
        "\n",
        "display(Audio('output.wav', autoplay=True))"
      ],
      "metadata": {
        "id": "Z56cvSBJKRSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tacotron2 is a popular tool for multilingual speech models"
      ],
      "metadata": {
        "id": "gDvKFRnAKv4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tts = TTS('tts_models/es/mai/tacotron2-DDC') # This is a model for a female spanish voice\n",
        "\n",
        "# Generate some speech\n",
        "tts.tts_to_file(\"Soy una lingüista apasionada. ¡Me encanta la fonética!\", file_path=\"output.wav\")\n",
        "\n",
        "display(Audio('output.wav', autoplay=True))"
      ],
      "metadata": {
        "id": "wqxmLLz2KqYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noise reduction with Speech Brain\n",
        "\n",
        "An open-source and all-in-one conversational AI toolkit based on PyTorch.\n",
        "\n",
        "Code source: https://colab.research.google.com/github/speechbrain/speechbrain/blob/develop/docs/tutorials/basics/what-can-i-do-with-speechbrain.ipynb#scrollTo=PuVNyffAhVfx"
      ],
      "metadata": {
        "id": "BVcncqidevdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installing SpeechBrain via pip\n",
        "BRANCH = 'develop'\n",
        "!python -m pip install git+https://github.com/speechbrain/speechbrain.git@$BRANCH\n"
      ],
      "metadata": {
        "id": "2LWQEwguhzf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import speechbrain as sb\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "from IPython.display import Audio"
      ],
      "metadata": {
        "id": "HU-Us0--h630"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from speechbrain.inference.separation import SepformerSeparation as separator\n",
        "import torchaudio\n",
        "!wget -O example_whamr.wav \"https://www.dropbox.com/scl/fi/gxbtbf3c3hxr0y9dbf0nw/example_whamr.wav?rlkey=1wt5d49kjl36h0zypwrmsy8nz&dl=1\"\n",
        "\n",
        "model = separator.from_hparams(source=\"speechbrain/sepformer-whamr-enhancement\", savedir='pretrained_models/sepformer-whamr-enhancement4')\n",
        "enhanced_speech = model.separate_file(path='/content/example_whamr.wav')\n"
      ],
      "metadata": {
        "id": "9_rxJi2Bh97V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signal = read_audio(\"/content/example_whamr.wav\").squeeze()\n",
        "Audio(signal, rate=8000)"
      ],
      "metadata": {
        "id": "SoxVa1UAjhSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(enhanced_speech[:, :].detach().cpu().squeeze(), rate=8000)"
      ],
      "metadata": {
        "id": "IKdWst4fjh-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speech separation with Speech Brain\n"
      ],
      "metadata": {
        "id": "-trF_W6EnBfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from speechbrain.inference.separation import SepformerSeparation as separator\n",
        "\n",
        "model = separator.from_hparams(source=\"speechbrain/sepformer-wsj02mix\", savedir='pretrained_models/sepformer-wsj02mix')\n",
        "!wget -O test_mixture.wav \"https://www.dropbox.com/scl/fi/4327g66ajs8aq3dck0fzn/test_mixture.wav?rlkey=bjdcw3msxw3armpelxuayug5i&dl=1\"\n",
        "est_sources = model.separate_file(path='/content/test_mixture.wav')"
      ],
      "metadata": {
        "id": "dQFcWbG5nTeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signal = read_audio(\"/content/test_mixture.wav\").squeeze()\n",
        "Audio(signal, rate=8000)"
      ],
      "metadata": {
        "id": "j-I49IuRnTv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(est_sources[:, :, 0].detach().cpu().squeeze(), rate=8000)"
      ],
      "metadata": {
        "id": "dP6LjLKQnT5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(est_sources[:, :, 1].detach().cpu().squeeze(), rate=8000)"
      ],
      "metadata": {
        "id": "e_5BzHCunT8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emotion recognition"
      ],
      "metadata": {
        "id": "CP5RsdSiousR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from speechbrain.inference.interfaces import foreign_class\n",
        "classifier = foreign_class(source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", pymodule_file=\"custom_interface.py\", classname=\"CustomEncoderWav2vec2Classifier\")\n"
      ],
      "metadata": {
        "id": "IMp_DO6vouBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify a sound file and print label:\n",
        "!wget -O example_whamr.wav \"https://www.dropbox.com/scl/fi/gxbtbf3c3hxr0y9dbf0nw/example_whamr.wav?rlkey=1wt5d49kjl36h0zypwrmsy8nz&dl=1\"\n",
        "\n",
        "out_prob, score, index, text_lab = classifier.classify_file(\"/content/example_whamr.wav\")\n",
        "print(text_lab)"
      ],
      "metadata": {
        "id": "5PuCF0r7tAaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speaker Verification\n",
        "The task here is to determine whether two sentences belong to the same speaker or not."
      ],
      "metadata": {
        "id": "c1-KKqrDoyEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O example1.wav \"https://www.dropbox.com/scl/fi/mu1tdejny4cbgxczwm944/example1.wav?rlkey=8pi7hjz15syvav80u1xzfbfhn&dl=1\"\n",
        "!wget -O example2.flac \"https://www.dropbox.com/scl/fi/k9ouk6ec1q1fkevamodrn/example2.flac?rlkey=vtbyc6bzp9hknzvn9rb63z3yf&dl=1\"\n",
        "\n",
        "from speechbrain.inference.speaker import SpeakerRecognition\n",
        "verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n",
        "score, prediction = verification.verify_files(\"/content/example1.wav\", \"/content/example2.flac\")"
      ],
      "metadata": {
        "id": "c_Fdn8aMtauN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signal = read_audio(\"/content/example1.wav\").squeeze()\n",
        "Audio(signal, rate=16000)"
      ],
      "metadata": {
        "id": "-yBzZIoDtazr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signal = read_audio(\"/content/example2.flac\").squeeze()\n",
        "Audio(signal, rate=16000)"
      ],
      "metadata": {
        "id": "SOuaWxdhta3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"prediction: {prediction[0]}, score: {score[0]}\")"
      ],
      "metadata": {
        "id": "cFaGfeYSt3h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References, games, demos\n",
        "\n",
        "- [An Easy Introduction to Speech AI](https://developer.nvidia.com/blog/an-easy-introduction-to-speech-ai/)\n",
        "- [Speech Brain Documentation](https://speechbrain.readthedocs.io/en/latest/API/speechbrain.html)\n",
        "- [Speech Brain Tutorial](https://colab.research.google.com/github/speechbrain/speechbrain/blob/develop/docs/tutorials/basics/what-can-i-do-with-speechbrain.ipynb#scrollTo=eto6x24aKo7e)\n",
        "- [Building a speaker recognition model](https://domino.ai/blog/building-a-speaker-recognition-model)\n",
        "- [Speech Recognition Technology: The Past, Present, and Future](https://medium.com/swlh/the-past-present-and-future-of-speech-recognition-technology-cf13c179aaf)\n",
        "- [Speech Technology: Theory and Applications](https://link.springer.com/book/10.1007/978-0-387-73819-2)\n",
        "- [Hidden Markov Models](https://web.stanford.edu/~jurafsky/slp3/16.pdf)\n",
        "- [Phonetics](https://web.stanford.edu/~jurafsky/slp3/H.pdf)\n",
        "- [Automatic Speech Recognition and Text to Speech](https://web.stanford.edu/~jurafsky/slp3/A.pdf)\n",
        "- [Gramle- spectrogram-based Wordle](https://nascl.rc.nau.edu/gramle/)\n"
      ],
      "metadata": {
        "id": "zMD4h5OF-NPV"
      }
    }
  ]
}